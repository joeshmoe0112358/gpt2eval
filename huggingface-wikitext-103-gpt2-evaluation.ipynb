{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\n\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast\nfrom datasets import load_dataset\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodels = {\n    \"small\": \"openai-community/gpt2\",\n    \"medium\": \"openai-community/gpt2-medium\",\n    \"large\": \"openai-community/gpt2-large\",\n    \"xl\": \"openai-community/gpt2-xl\"\n}\n\nprint(\"Loading dataset...\")\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\ntest = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"validation\")\nencodings = tokenizer(\"\".join(test[\"text\"]), return_tensors=\"pt\")\nprint(\"Dataset loaded\")\n\nfor size in models:\n    print(f\"Loading gpt2-{size}...\")\n    model = GPT2LMHeadModel.from_pretrained(models[size]).to(device)\n    print(f\"gpt2-{size} loaded\")\n\n    max_length = model.config.n_positions\n    stride = max_length\n    seq_len = encodings.input_ids.size(1)\n\n    nlls = []\n    prev_end_loc = 0\n    for begin_loc in tqdm(range(0, seq_len, stride), desc = f\"Evaluating gpt2-{size}\"):\n        end_loc = min(begin_loc + max_length, seq_len)\n        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n        target_ids = input_ids.clone()\n        target_ids[:, :-trg_len] = -100\n\n        with torch.no_grad():\n            outputs = model(input_ids, labels = target_ids)\n\n            # loss is calculated using CrossEntropyLoss which averages over valid labels\n            # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n            # to the left by 1.\n            neg_log_likelihood = outputs.loss\n\n        nlls.append(neg_log_likelihood)\n\n        prev_end_loc = end_loc\n        if end_loc == seq_len:\n            break\n\n    ppl = torch.exp(torch.stack(nlls).mean())\n    print(f\"gpt2-{size} perplexity score:\", ppl.item())","metadata":{"execution":{"iopub.status.busy":"2024-05-03T02:47:54.590527Z","iopub.execute_input":"2024-05-03T02:47:54.591275Z","iopub.status.idle":"2024-05-03T02:54:36.492836Z","shell.execute_reply.started":"2024-05-03T02:47:54.591219Z","shell.execute_reply":"2024-05-03T02:54:36.491876Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Loading dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bb0407dd4ba44c1991064bdeb2845ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2ffe8327aa540a4932930c9443de223"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fab49c9c4ae4939981e6dc325134d11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"161ebc2f958c4407af4db39ee83b4a32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"013d6cb28ab042bc87beb3287ddeacdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55f09ee1d38049b3b49b3f1958e53f34"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 733k/733k [00:00<00:00, 2.63MB/s]\nDownloading data: 100%|██████████| 157M/157M [00:03<00:00, 44.6MB/s] \nDownloading data: 100%|██████████| 157M/157M [00:03<00:00, 44.0MB/s] \nDownloading data: 100%|██████████| 657k/657k [00:00<00:00, 3.80MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc3b63275fde49c69c1b1b7a47069812"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cc7c370a93940fe932e1b5c6cf7eb87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccdc8be089204a9b9a39efc408b716ed"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (247289 > 1024). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"Dataset loaded\nLoading gpt2-small...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc23e3ec397447189049ce9ce1f1ef40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"061649eefa7c4d2991e16200b46ec2bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"545316f045074654ab40a4b2f478460d"}},"metadata":{}},{"name":"stdout","text":"gpt2-small loaded\n","output_type":"stream"},{"name":"stderr","text":"Evaluating gpt2-small: 100%|█████████▉| 241/242 [00:14<00:00, 16.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"gpt2-small perplexity score: 30.58563804626465\nLoading gpt2-medium...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d51e8e2854eb4d4eb30ee89493d89d49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fe3d6bef0e44aaca968288bb0ccc9f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f403c03a52c248ec92dcbe2eb9a9d4bc"}},"metadata":{}},{"name":"stdout","text":"gpt2-medium loaded\n","output_type":"stream"},{"name":"stderr","text":"Evaluating gpt2-medium: 100%|█████████▉| 241/242 [00:38<00:00,  6.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"gpt2-medium perplexity score: 22.348369598388672\nLoading gpt2-large...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f94a52fe63b4c5eb9cb58e6b3c470c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69d5c6535f9d41a1825fdc00797df8b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3065284d83c4411a2bb9ebfb6fba189"}},"metadata":{}},{"name":"stdout","text":"gpt2-large loaded\n","output_type":"stream"},{"name":"stderr","text":"Evaluating gpt2-large: 100%|█████████▉| 241/242 [01:20<00:00,  3.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"gpt2-large perplexity score: 19.330533981323242\nLoading gpt2-xl...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b6f9c59a6074b1db44a2366ea01854d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdce5d6ec4174c84b047f292d060789f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4bc927e38cc40fcb2245f1e28a7822e"}},"metadata":{}},{"name":"stdout","text":"gpt2-xl loaded\n","output_type":"stream"},{"name":"stderr","text":"Evaluating gpt2-xl: 100%|█████████▉| 241/242 [02:33<00:00,  1.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"gpt2-xl perplexity score: 17.458951950073242\n","output_type":"stream"}]},{"cell_type":"code","source":"# the difference between the previous cell and this cell is in the joining of the test text by \"\\n\\n\"\nimport os\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\n\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast\nfrom datasets import load_dataset\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodels = {\n    \"small\": \"openai-community/gpt2\",\n    \"medium\": \"openai-community/gpt2-medium\",\n    \"large\": \"openai-community/gpt2-large\",\n    \"xl\": \"openai-community/gpt2-xl\"\n}\n\nprint(\"Loading dataset...\")\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\ntest = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"validation\")\nencodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")\nprint(\"Dataset loaded\")\n\nfor size in models:\n    print(f\"Loading gpt2-{size}...\")\n    model = GPT2LMHeadModel.from_pretrained(models[size]).to(device)\n    print(f\"gpt2-{size} loaded\")\n\n    max_length = model.config.n_positions\n    stride = max_length\n    seq_len = encodings.input_ids.size(1)\n\n    nlls = []\n    prev_end_loc = 0\n    for begin_loc in tqdm(range(0, seq_len, stride), desc = f\"Evaluating gpt2-{size}\"):\n        end_loc = min(begin_loc + max_length, seq_len)\n        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n        target_ids = input_ids.clone()\n        target_ids[:, :-trg_len] = -100\n\n        with torch.no_grad():\n            outputs = model(input_ids, labels = target_ids)\n\n            # loss is calculated using CrossEntropyLoss which averages over valid labels\n            # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n            # to the left by 1.\n            neg_log_likelihood = outputs.loss\n\n        nlls.append(neg_log_likelihood)\n\n        prev_end_loc = end_loc\n        if end_loc == seq_len:\n            break\n\n    ppl = torch.exp(torch.stack(nlls).mean())\n    print(f\"gpt2-{size} perplexity score:\", ppl.item())","metadata":{"execution":{"iopub.status.busy":"2024-05-03T02:56:01.240211Z","iopub.execute_input":"2024-05-03T02:56:01.240863Z","iopub.status.idle":"2024-05-03T03:01:22.626872Z","shell.execute_reply.started":"2024-05-03T02:56:01.240830Z","shell.execute_reply":"2024-05-03T03:01:22.625765Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Loading dataset...\n","output_type":"stream"},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (251048 > 1024). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"Dataset loaded\nLoading gpt2-small...\ngpt2-small loaded\n","output_type":"stream"},{"name":"stderr","text":"Evaluating gpt2-small: 100%|█████████▉| 245/246 [00:14<00:00, 16.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"gpt2-small perplexity score: 31.042251586914062\nLoading gpt2-medium...\ngpt2-medium loaded\n","output_type":"stream"},{"name":"stderr","text":"Evaluating gpt2-medium: 100%|█████████▉| 245/246 [00:39<00:00,  6.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"gpt2-medium perplexity score: 22.5109920501709\nLoading gpt2-large...\ngpt2-large loaded\n","output_type":"stream"},{"name":"stderr","text":"Evaluating gpt2-large: 100%|█████████▉| 245/246 [01:21<00:00,  3.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"gpt2-large perplexity score: 20.088329315185547\nLoading gpt2-xl...\ngpt2-xl loaded\n","output_type":"stream"},{"name":"stderr","text":"Evaluating gpt2-xl: 100%|█████████▉| 245/246 [02:35<00:00,  1.57it/s]","output_type":"stream"},{"name":"stdout","text":"gpt2-xl perplexity score: 17.91388702392578\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}