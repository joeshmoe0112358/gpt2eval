{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tiktoken","metadata":{"execution":{"iopub.status.busy":"2024-05-03T03:29:22.301127Z","iopub.execute_input":"2024-05-03T03:29:22.301522Z","iopub.status.idle":"2024-05-03T03:29:37.998372Z","shell.execute_reply.started":"2024-05-03T03:29:22.301491Z","shell.execute_reply":"2024-05-03T03:29:37.997131Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Collecting tiktoken\n  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2023.12.25)\nRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\nDownloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tiktoken\nSuccessfully installed tiktoken-0.6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport re\nimport requests\nimport argparse\nimport zipfile\nfrom tqdm import tqdm\n\nimport tiktoken\nimport numpy as np\n\nDATA_CACHE_DIR = \"/kaggle/working/data\"\nenc = tiktoken.get_encoding(\"gpt2\")\nencode = lambda s: enc.encode(s, allowed_special = {\"<|endoftext|>\"})\n\ndef download_file(url : str, fname : str, chunk_size = 1024):\n    \"\"\"Helper function to download a file from a given url\"\"\"\n    resp = requests.get(url, stream = True)\n    total = int(resp.headers.get(\"content-length\", 0))\n    with open(fname, \"wb\") as file, tqdm(\n        desc = fname,\n        total = total,\n        unit = \"iB\",\n        unit_scale = True,\n        unit_divisor = 1024,\n    ) as bar:\n        for data in resp.iter_content(chunk_size = chunk_size):\n            size = file.write(data)\n            bar.update(size)\n            \ndef download():\n    \"\"\"Downloads the WikiText-103 dataset to DATA_CACHE_DIR\"\"\"\n    os.makedirs(DATA_CACHE_DIR, exist_ok = True)\n\n    # download the WikiText-103 dataset, unless it's already downloaded\n    data_url = \"https://wikitext.smerity.com/wikitext-103-raw-v1.zip\"\n    data_filename = os.path.join(DATA_CACHE_DIR, \"WikiText-103.zip\")\n    if not os.path.exists(data_filename):\n        print(f\"Downloading {data_url} to {data_filename}...\")\n        download_file(data_url, data_filename)\n    else:\n        print(f\"{data_filename} already exists, skipping download...\")\n\n    # unzip the file\n    data_dir = os.path.join(DATA_CACHE_DIR, \"wikitext-103\")\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir, exist_ok = True)\n        print(f\"Unzipping {data_filename}...\")\n        with zipfile.ZipFile(data_filename, \"r\") as zip_ref:\n            zip_ref.extractall(data_dir)\n    else:\n        print(f\"{data_dir} already exists, skipping unzipping...\")\n\ndef tokenize(preprocess : bool):\n    # special token\n    eot = enc._special_tokens[\"<|endoftext|>\"]\n\n    # fetch validation text\n    val_data_filename = os.path.join(DATA_CACHE_DIR, \"wikitext-103/wikitext-103-raw/wiki.valid.raw\")\n    val_text = open(val_data_filename, \"r\", encoding = \"utf-8\").read()\n\n    if preprocess:\n        print(\"Cleaning validation data...\")\n        # cleanup the training text\n        val_text = val_text.strip() # remove leading and trailing whitespace\n        val_text = val_text.replace(\" \\n \\n \", \"\\n<|endoftext|>\") # injecting special token in between sections\n        val_text = val_text.replace(\"@-@\", \"-\")\n        val_text = val_text.replace(\"@.@\", \".\")\n        val_text = val_text.replace(\"@,@\", \",\")\n        val_text = \"<|endoftext|>\" + val_text # adding special token at start\n        val_split = val_text.split(\"<|endoftext|>\") # splitting the text by special token to remove the extraneous headers/titles\n\n        # remove the awkward headers/titles that came from the original parquet format\n        for chunk in tqdm([item for item in reversed(range(len(val_split)))], desc = \"Removing artifacts\", unit = \"iB\"):\n            # if the chunk is of the form of the headers/titles we will pop this chunk out\n            if bool(re.match(r\"^\\s*= +(.{1,}) +=\\s*$\", val_split[chunk])):\n                val_split.pop(chunk)\n\n        # now join the remaining chunks via eot\n        val_text = \"<|endoftext|>\".join(val_split[i] for i in range(len(val_split)))\n    print(\"Tokenizing validation text...\")\n    val_tokens = encode(val_text)\n    print(\"Validation text tokenized\")\n    val_tokens_np = np.array(val_tokens, dtype = np.int32)\n\n    print(\"Dumping text into text files to observe readable output\")\n    with open(os.path.join(DATA_CACHE_DIR, \"wikitext-103-preprocessed_val.txt\" if preprocess else \"wikitext-103-raw_val.txt\"), \"w\") as f:\n        f.write(val_text)\n\n    # now just dump the encoded tokens into binary files\n    val_filename = os.path.join(DATA_CACHE_DIR, \"wikitext-103-preprocessed_val.bin\" if preprocess else \"wikitext-103-raw_val.bin\")\n\n    with open(val_filename, \"wb\") as f:\n        for chunk in tqdm([val_tokens_np[i : i + 1024] for i in range(0, len(val_tokens_np), 1024)], desc = \"Writing validation data to wikitext-103_val.bin\", unit = \"iB\"):\n            f.write(chunk.tobytes())\n    \n    print(f\"Saved {len(val_tokens_np)} tokens to {val_filename}\")\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-03T03:31:29.811667Z","iopub.execute_input":"2024-05-03T03:31:29.812073Z","iopub.status.idle":"2024-05-03T03:31:29.834218Z","shell.execute_reply.started":"2024-05-03T03:31:29.812042Z","shell.execute_reply":"2024-05-03T03:31:29.833189Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"download()\ntokenize(True)\ntokenize(False)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T03:31:30.096885Z","iopub.execute_input":"2024-05-03T03:31:30.097281Z","iopub.status.idle":"2024-05-03T03:31:30.625221Z","shell.execute_reply.started":"2024-05-03T03:31:30.097250Z","shell.execute_reply":"2024-05-03T03:31:30.624199Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"/kaggle/working/data/WikiText-103.zip already exists, skipping download...\n/kaggle/working/data/wikitext-103 already exists, skipping unzipping...\nCleaning validation data...\n","output_type":"stream"},{"name":"stderr","text":"Removing artifacts: 100%|██████████| 1161/1161 [00:00<00:00, 286766.79iB/s]","output_type":"stream"},{"name":"stdout","text":"Tokenizing validation text...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation text tokenized\nDumping text into text files to observe readable output\n","output_type":"stream"},{"name":"stderr","text":"Writing validation data to wikitext-103_val.bin: 100%|██████████| 231/231 [00:00<00:00, 91300.81iB/s]","output_type":"stream"},{"name":"stdout","text":"Saved 236191 tokens to /kaggle/working/data/wikitext-103-preprocessed_val.bin\nTokenizing validation text...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation text tokenized\nDumping text into text files to observe readable output\n","output_type":"stream"},{"name":"stderr","text":"Writing validation data to wikitext-103_val.bin: 100%|██████████| 245/245 [00:00<00:00, 76835.99iB/s]","output_type":"stream"},{"name":"stdout","text":"Saved 249887 tokens to /kaggle/working/data/wikitext-103-raw_val.bin\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# on raw wikitext-103\nimport os\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\n\nfrom transformers import GPT2LMHeadModel\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodels = {\n    \"small\": \"openai-community/gpt2\",\n    \"medium\": \"openai-community/gpt2-medium\",\n    \"large\": \"openai-community/gpt2-large\",\n    \"xl\": \"openai-community/gpt2-xl\"\n}\n\nprint(\"Loading dataset...\")\nwith open(\"/kaggle/working/data/wikitext-103-raw_val.bin\", \"rb\") as f:\n    eval_text = np.frombuffer(f.read(), dtype=np.int32)\n    eval_text = torch.tensor(eval_text, dtype = torch.long).unsqueeze(0)\nprint(\"Dataset loaded\")\n\nfor size in models:\n    print(f\"Loading gpt2-{size}...\")\n    model = GPT2LMHeadModel.from_pretrained(models[size]).to(device)\n    print(f\"gpt2-{size} loaded\")\n\n    max_length = model.config.n_positions\n    stride = max_length\n    seq_len = eval_text.shape[1]\n\n    nlls = []\n    prev_end_loc = 0\n    for begin_loc in tqdm(range(0, seq_len, stride), desc = f\"Evaluating gpt2-{size}\"):\n        end_loc = min(begin_loc + max_length, seq_len)\n        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n        input_ids = eval_text[:, begin_loc:end_loc].to(device)\n        target_ids = input_ids.clone()\n        target_ids[:, :-trg_len] = -100\n\n        with torch.no_grad():\n            outputs = model(input_ids, labels = target_ids)\n\n            # loss is calculated using CrossEntropyLoss which averages over valid labels\n            # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n            # to the left by 1.\n            neg_log_likelihood = outputs.loss\n\n        nlls.append(neg_log_likelihood)\n\n        prev_end_loc = end_loc\n        if end_loc == seq_len:\n            break\n\n    ppl = torch.exp(torch.stack(nlls).mean())\n    print(f\"gpt2-{size} perplexity score:\", ppl.item())","metadata":{"execution":{"iopub.status.busy":"2024-05-03T03:33:30.313253Z","iopub.execute_input":"2024-05-03T03:33:30.314164Z","iopub.status.idle":"2024-05-03T03:40:06.958455Z","shell.execute_reply.started":"2024-05-03T03:33:30.314131Z","shell.execute_reply":"2024-05-03T03:40:06.957344Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Loading dataset...\nDataset loaded\nLoading gpt2-small...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8995f286764425393b7a5f349050685"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e7ed303333d4fb18bf43603678288b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3758b45dbc6d4aa395f3a47e31581957"}},"metadata":{}},{"name":"stdout","text":"gpt2-small loaded\n","output_type":"stream"},{"name":"stderr","text":"Evaluating gpt2-small: 100%|█████████▉| 244/245 [00:15<00:00, 16.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"gpt2-small perplexity score: 30.130029678344727\nLoading gpt2-medium...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54749198305746a0810b3eb6db746ce7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89598ae81c8d466ba46a1b3caca0618c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f0a576afff84d57b7d357f81d739aae"}},"metadata":{}},{"name":"stdout","text":"gpt2-medium loaded\n","output_type":"stream"},{"name":"stderr","text":"Evaluating gpt2-medium: 100%|█████████▉| 244/245 [00:38<00:00,  6.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"gpt2-medium perplexity score: 21.772069931030273\nLoading gpt2-large...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ba545284f464907adcdb486cff7f87d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a04c9b26f274cbf844ee1e5d43dab06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14fa224082844d5b830ea887d0efdd67"}},"metadata":{}},{"name":"stdout","text":"gpt2-large loaded\n","output_type":"stream"},{"name":"stderr","text":"Evaluating gpt2-large: 100%|█████████▉| 244/245 [01:21<00:00,  3.01it/s]","output_type":"stream"},{"name":"stdout","text":"gpt2-large perplexity score: 18.740623474121094\nLoading gpt2-xl...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1399946ac704f97852fee5b7d72c81f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57b0d911db9540eebb80a38d55c47c43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e325eda08188476eaacffe1a8ad8cc31"}},"metadata":{}},{"name":"stdout","text":"gpt2-xl loaded\n","output_type":"stream"},{"name":"stderr","text":"Evaluating gpt2-xl: 100%|█████████▉| 244/245 [02:34<00:00,  1.58it/s]","output_type":"stream"},{"name":"stdout","text":"gpt2-xl perplexity score: 16.912160873413086\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# on my preprocessed wikitext-103\nimport os\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\n\nfrom transformers import GPT2LMHeadModel\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodels = {\n    \"small\": \"openai-community/gpt2\",\n    \"medium\": \"openai-community/gpt2-medium\",\n    \"large\": \"openai-community/gpt2-large\",\n    \"xl\": \"openai-community/gpt2-xl\"\n}\n\nprint(\"Loading dataset...\")\nwith open(\"/kaggle/working/data/wikitext-103-preprocessed_val.bin\", \"rb\") as f:\n    eval_text = np.frombuffer(f.read(), dtype=np.int32)\n    eval_text = torch.tensor(eval_text, dtype = torch.long).unsqueeze(0)\nprint(\"Dataset loaded\")\n\nfor size in models:\n    print(f\"Loading gpt2-{size}...\")\n    model = GPT2LMHeadModel.from_pretrained(models[size]).to(device)\n    print(f\"gpt2-{size} loaded\")\n\n    max_length = model.config.n_positions\n    stride = max_length\n    seq_len = eval_text.shape[1]\n\n    nlls = []\n    prev_end_loc = 0\n    for begin_loc in tqdm(range(0, seq_len, stride), desc = f\"Evaluating gpt2-{size}\"):\n        end_loc = min(begin_loc + max_length, seq_len)\n        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n        input_ids = eval_text[:, begin_loc:end_loc].to(device)\n        target_ids = input_ids.clone()\n        target_ids[:, :-trg_len] = -100\n\n        with torch.no_grad():\n            outputs = model(input_ids, labels = target_ids)\n\n            # loss is calculated using CrossEntropyLoss which averages over valid labels\n            # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n            # to the left by 1.\n            neg_log_likelihood = outputs.loss\n\n        nlls.append(neg_log_likelihood)\n\n        prev_end_loc = end_loc\n        if end_loc == seq_len:\n            break\n\n    ppl = torch.exp(torch.stack(nlls).mean())\n    print(f\"gpt2-{size} perplexity score:\", ppl.item())","metadata":{"execution":{"iopub.status.busy":"2024-05-03T03:40:06.960816Z","iopub.execute_input":"2024-05-03T03:40:06.961634Z","iopub.status.idle":"2024-05-03T03:45:11.646748Z","shell.execute_reply.started":"2024-05-03T03:40:06.961591Z","shell.execute_reply":"2024-05-03T03:45:11.645596Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Loading dataset...\nDataset loaded\nLoading gpt2-small...\ngpt2-small loaded\n","output_type":"stream"},{"name":"stderr","text":"Evaluating gpt2-small: 100%|█████████▉| 230/231 [00:13<00:00, 16.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"gpt2-small perplexity score: 33.19379425048828\nLoading gpt2-medium...\ngpt2-medium loaded\n","output_type":"stream"},{"name":"stderr","text":"Evaluating gpt2-medium: 100%|█████████▉| 230/231 [00:36<00:00,  6.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"gpt2-medium perplexity score: 24.309528350830078\nLoading gpt2-large...\ngpt2-large loaded\n","output_type":"stream"},{"name":"stderr","text":"Evaluating gpt2-large: 100%|█████████▉| 230/231 [01:16<00:00,  3.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"gpt2-large perplexity score: 21.38955307006836\nLoading gpt2-xl...\ngpt2-xl loaded\n","output_type":"stream"},{"name":"stderr","text":"Evaluating gpt2-xl: 100%|█████████▉| 230/231 [02:26<00:00,  1.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"gpt2-xl perplexity score: 19.324647903442383\n","output_type":"stream"}]}]}